{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch==1.6.0\n",
      "  Downloading torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (748.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 748.8 MB 16 kB/s \n",
      "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0) (0.18.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0) (1.17.4)\n",
      "Installing collected packages: torch\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/home/user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed torch-1.6.0\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.4.0.46-cp36-cp36m-manylinux2014_x86_64.whl (49.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 49.5 MB 29.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.17.4)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.4.0.46\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchvision==0.2.2\n",
      "  Downloading torchvision-0.2.2-py2.py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 5.6 MB/s \n",
      "\u001b[?25hCollecting tqdm==4.19.9\n",
      "  Downloading tqdm-4.19.9-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 3.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.2) (1.17.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.2) (1.11.0)\n",
      "Requirement already satisfied: torch in /home/user/.local/lib/python3.6/site-packages (from torchvision==0.2.2) (1.6.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.2) (7.2.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==0.2.2) (0.18.2)\n",
      "Installing collected packages: tqdm, torchvision\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/home/user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed torchvision-0.2.2 tqdm-4.19.9\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting albumentations\n",
      "  Downloading albumentations-0.5.2-py3-none-any.whl (72 kB)\n",
      "\u001b[K     |████████████████████████████████| 72 kB 1.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.17.4)\n",
      "Collecting opencv-python-headless>=4.1.1\n",
      "  Downloading opencv_python_headless-4.4.0.46-cp36-cp36m-manylinux2014_x86_64.whl (36.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 36.7 MB 9.1 MB/s \n",
      "\u001b[?25hCollecting imgaug>=0.4.0\n",
      "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
      "\u001b[K     |████████████████████████████████| 948 kB 14.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.1.0)\n",
      "Collecting scikit-image>=0.16.1\n",
      "  Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 17.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations) (5.3.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.11.0)\n",
      "Collecting imageio\n",
      "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 9.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (7.2.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (3.0.0)\n",
      "Collecting Shapely\n",
      "  Downloading Shapely-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 15.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: opencv-python in /home/user/.local/lib/python3.6/site-packages (from imgaug>=0.4.0->albumentations) (4.4.0.46)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5)\n",
      "Collecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 27.9 MB/s \n",
      "\u001b[?25hCollecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2020.9.3-py3-none-any.whl (148 kB)\n",
      "\u001b[K     |████████████████████████████████| 148 kB 44.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.7.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->imgaug>=0.4.0->albumentations) (49.6.0)\n",
      "Installing collected packages: opencv-python-headless, imageio, PyWavelets, tifffile, scikit-image, Shapely, imgaug, albumentations\n",
      "\u001b[33m  WARNING: The scripts imageio_download_bin and imageio_remove_bin are installed in '/home/user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts lsm2bin and tifffile are installed in '/home/user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script skivi is installed in '/home/user/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "scikit-image 0.17.2 requires matplotlib!=3.0.0,>=2.0.0, but you'll have matplotlib 3.0.0 which is incompatible.\u001b[0m\n",
      "Successfully installed PyWavelets-1.1.1 Shapely-1.7.1 albumentations-0.5.2 imageio-2.9.0 imgaug-0.4.0 opencv-python-headless-4.4.0.46 scikit-image-0.17.2 tifffile-2020.9.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.6.0\n",
    "!pip install opencv-python\n",
    "!pip install torchvision==0.2.2\n",
    "!pip install albumentations\n",
    "!pip install tensorflow\n",
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import typing as tp\n",
    "import yaml\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "# import resnest.torch as resnest_torch\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "# from resnet import ResNet, Bottleneck\n",
    "\n",
    "from albumentations.core.transforms_interface import DualTransform, BasicTransform\n",
    "import albumentations as albu\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_set = {\n",
    "    'dataset': {\n",
    "          'name': 'SpectrogramDataset',\n",
    "          'params': {\n",
    "            'img_size': 224, \n",
    "            'melspectrogram_parameters': {\n",
    "              'n_mels': 128, \n",
    "              'fmin': 50, \n",
    "              'fmax': 15000, \n",
    "            }\n",
    "      }\n",
    "    },\n",
    "    'loader': {\n",
    "      'train': {\n",
    "        'batch_size': 6,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 2,\n",
    "        'pin_memory': True,\n",
    "        'drop_last': True,\n",
    "      },\n",
    "      'valid': {\n",
    "        'batch_size': 2,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 2,\n",
    "        'pin_memory': True,\n",
    "        'drop_last': True,\n",
    "      }\n",
    "    }\n",
    "}\n",
    "SEED=100\n",
    "PERIOD = 5\n",
    "SPECIES_NUM = 24\n",
    "EPOCH = 50\n",
    "OUTPUT_DIR = './output/'\n",
    "HOP_LEN = 512\n",
    "SR = 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_ROOT = Path(\"/home/knikaido/work/Rainforest-Connection/data\")\n",
    "RAW_DATA = INPUT_ROOT / \"rfcx-species-audio-detection\"\n",
    "TRAIN_AUDIO_DIR = RAW_DATA / \"train\"\n",
    "# TRAIN_RESAMPLED_AUDIO_DIRS = [\n",
    "#   INPUT_ROOT / \"birdsong-resampled-train-audio-{:0>2}\".format(i)  for i in range(5)\n",
    "# ]\n",
    "TEST_AUDIO_DIR = RAW_DATA / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recording_id</th>\n",
       "      <th>species_id</th>\n",
       "      <th>songtype_id</th>\n",
       "      <th>t_min</th>\n",
       "      <th>f_min</th>\n",
       "      <th>t_max</th>\n",
       "      <th>f_max</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>003bec244</td>\n",
       "      <td>[14]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[44.544]</td>\n",
       "      <td>[2531.25]</td>\n",
       "      <td>[45.1307]</td>\n",
       "      <td>[5531.25]</td>\n",
       "      <td>/home/knikaido/work/Rainforest-Connection/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>006ab765f</td>\n",
       "      <td>[23]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[39.9615]</td>\n",
       "      <td>[7235.16]</td>\n",
       "      <td>[46.0452]</td>\n",
       "      <td>[11283.4]</td>\n",
       "      <td>/home/knikaido/work/Rainforest-Connection/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>007f87ba2</td>\n",
       "      <td>[12]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[39.135999999999996]</td>\n",
       "      <td>[562.5]</td>\n",
       "      <td>[42.272]</td>\n",
       "      <td>[3281.25]</td>\n",
       "      <td>/home/knikaido/work/Rainforest-Connection/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0099c367b</td>\n",
       "      <td>[17]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[51.4206]</td>\n",
       "      <td>[1464.26]</td>\n",
       "      <td>[55.1996]</td>\n",
       "      <td>[4565.04]</td>\n",
       "      <td>/home/knikaido/work/Rainforest-Connection/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009b760e6</td>\n",
       "      <td>[10]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[50.0854]</td>\n",
       "      <td>[947.461]</td>\n",
       "      <td>[52.5293]</td>\n",
       "      <td>[10852.7]</td>\n",
       "      <td>/home/knikaido/work/Rainforest-Connection/data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  recording_id species_id songtype_id                 t_min      f_min  \\\n",
       "0    003bec244       [14]         [1]              [44.544]  [2531.25]   \n",
       "1    006ab765f       [23]         [1]             [39.9615]  [7235.16]   \n",
       "2    007f87ba2       [12]         [1]  [39.135999999999996]    [562.5]   \n",
       "3    0099c367b       [17]         [4]             [51.4206]  [1464.26]   \n",
       "4    009b760e6       [10]         [1]             [50.0854]  [947.461]   \n",
       "\n",
       "       t_max      f_max                                               name  \n",
       "0  [45.1307]  [5531.25]  /home/knikaido/work/Rainforest-Connection/data...  \n",
       "1  [46.0452]  [11283.4]  /home/knikaido/work/Rainforest-Connection/data...  \n",
       "2   [42.272]  [3281.25]  /home/knikaido/work/Rainforest-Connection/data...  \n",
       "3  [55.1996]  [4565.04]  /home/knikaido/work/Rainforest-Connection/data...  \n",
       "4  [52.5293]  [10852.7]  /home/knikaido/work/Rainforest-Connection/data...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gby = pd.read_pickle(RAW_DATA / \"train_gby.pkl\")\n",
    "train_gby.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mono_to_color(\n",
    "    X: np.ndarray, mean=None, std=None,\n",
    "    norm_max=None, norm_min=None, eps=1e-6\n",
    "):\n",
    "    # Stack X as [X,X,X]\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    X = X - mean\n",
    "    std = std or X.std()\n",
    "    Xstd = X / (std + eps)\n",
    "    _min, _max = Xstd.min(), Xstd.max()\n",
    "    norm_max = norm_max or _max\n",
    "    norm_min = norm_min or _min\n",
    "    if (_max - _min) > eps:\n",
    "        # Normalize to [0, 255]\n",
    "        V = Xstd\n",
    "        V[V < norm_min] = norm_min\n",
    "        V[V > norm_max] = norm_max\n",
    "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        # Just zero\n",
    "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramTrainDataset(data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gby_df: pd.DataFrame,\n",
    "        setting: tp.Dict\n",
    "    ):\n",
    "        self.img_size = setting['img_size']\n",
    "        self.melspectrogram_parameters = setting['melspectrogram_parameters']\n",
    "        \n",
    "        self.gby_df = gby_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gby_df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \n",
    "        wav_path = self.gby_df['name'][idx]\n",
    "        train_element = self.gby_df.iloc[idx]\n",
    "\n",
    "        y, sr = sf.read(wav_path)\n",
    "        \n",
    "        len_y = len(y)\n",
    "        effective_length = sr * PERIOD\n",
    "\n",
    "        tmin_list = train_element['t_min']\n",
    "        tmax_list = train_element['t_max']\n",
    "        \n",
    "        #時間かかる\n",
    "        end_flag = False\n",
    "        while(end_flag==False):\n",
    "            start = np.random.randint(len_y - effective_length)\n",
    "            end = start + effective_length\n",
    "            for i in range(len(tmin_list)):\n",
    "                tmin = float(tmin_list[i]) * sr\n",
    "                tmax = float(tmax_list[i]) * sr\n",
    "                if( (start < tmin and tmin < end) or (start < tmax and tmax < end) ):\n",
    "                    end_flag = True\n",
    "                    break\n",
    "\n",
    "#         tmin = float(tmin_list[0]) * sr\n",
    "#         start_min = max(0, tmin - effective_length)\n",
    "#         start_max = min(tmin, len_y - effective_length)\n",
    "#         start = np.random.randint(start_min, start_max)\n",
    "#         end = start + effective_length    \n",
    "\n",
    "        y = y[start:end].astype(np.float32)\n",
    "\n",
    "        melspec = librosa.feature.melspectrogram(y, sr=sr, **self.melspectrogram_parameters)\n",
    "        melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
    "\n",
    "        image = mono_to_color(melspec)\n",
    "        height, width, _ = image.shape\n",
    "        image = cv2.resize(image, (int(width * self.img_size / height), self.img_size))\n",
    "        image = np.moveaxis(image, 2, 0)\n",
    "        image = (image / 255.0).astype(np.float32)\n",
    "        \n",
    "        label = np.zeros(SPECIES_NUM, dtype=\"f\")\n",
    "        label[train_element['species_id'][0]] = 1\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_criterion():\n",
    "    pos_weights = torch.ones(SPECIES_NUM)\n",
    "    pos_weights = pos_weights * SPECIES_NUM\n",
    "    loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "    return loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='valid_epoch_loss',\n",
    "   min_delta=0.00,\n",
    "   patience=5,\n",
    "   verbose=True,\n",
    "   mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4167)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _one_sample_positive_class_precisions(scores, truth):\n",
    "    num_classes = scores.shape[0]\n",
    "    pos_class_indices = np.flatnonzero(truth > 0)\n",
    "\n",
    "    if not len(pos_class_indices):\n",
    "        return pos_class_indices, np.zeros(0)\n",
    "\n",
    "    retrieved_classes = np.argsort(scores)[::-1]\n",
    "\n",
    "    class_rankings = np.zeros(num_classes, dtype=np.int)\n",
    "    class_rankings[retrieved_classes] = range(num_classes)\n",
    "\n",
    "    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
    "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
    "\n",
    "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
    "\n",
    "    precision_at_hits = (\n",
    "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
    "            (1 + class_rankings[pos_class_indices].astype(np.float)))\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "def lwlrap(truth, scores):\n",
    "    assert truth.shape == scores.shape\n",
    "    num_samples, num_classes = scores.shape\n",
    "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
    "    for sample_num in range(num_samples):\n",
    "        pos_class_indices, precision_at_hits = _one_sample_positive_class_precisions(scores[sample_num, :], truth[sample_num, :])\n",
    "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = precision_at_hits\n",
    "\n",
    "    labels_per_class = np.sum(truth > 0, axis=0)\n",
    "    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
    "\n",
    "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
    "                        np.maximum(1, labels_per_class))\n",
    "    return per_class_lwlrap, weight_per_class\n",
    "\n",
    "def lwlap_wrapper(y_true, y_score):\n",
    "    y_true = y_true.to('cpu').detach().numpy().copy()\n",
    "    y_score = y_score.to('cpu').detach().numpy().copy()\n",
    "    score_class, weight = lwlrap(y_true, y_score)\n",
    "    score_class = torch.from_numpy(score_class.astype(np.float32)).clone()\n",
    "    weight = torch.from_numpy(weight.astype(np.float32)).clone()\n",
    "    return score_class, weight\n",
    "\n",
    "y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
    "y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
    "y_true = torch.from_numpy(y_true.astype(np.float32)).clone()\n",
    "y_score = torch.from_numpy(y_score.astype(np.float32)).clone()\n",
    "\n",
    "score_class, weight = lwlap_wrapper(y_true, y_score)\n",
    "score = (score_class * weight).sum()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModule(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)\n",
    "        self.encoder.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(1024, SPECIES_NUM)\n",
    "        )\n",
    "        \n",
    "        self.criterion = get_criterion()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_out = self.encoder(x)\n",
    "        return x_out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=0.0001, momentum=0.9)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        y_pred = self.encoder(x)    \n",
    "        loss = self.criterion(y_pred, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        y_pred = self.encoder(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        lwlap_step, weight_step = lwlap_wrapper(y, y_pred)\n",
    "        lwlap_step = (lwlap_step * weight_step).sum()\n",
    "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('lwlap_score', lwlap_step, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss, lwlap_step\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        validation_step_outputs = np.array(validation_step_outputs)\n",
    "        validation_step_losses = validation_step_outputs[:, 0]\n",
    "        mean_loss = torch.stack([x for x in validation_step_losses]).mean()\n",
    "        \n",
    "        validation_step_scores = validation_step_outputs[:, 1]\n",
    "        mean_score = torch.stack([x for x in validation_step_scores]).mean()\n",
    "\n",
    "        print('valid_epoch_loss = ', mean_loss)\n",
    "        print('valid_epoch_lwlap = ', mean_score)\n",
    "        self.log('valid_epoch_loss', mean_loss, prog_bar=True, logger=True)\n",
    "        self.log('valid_epoch_lwlap', mean_score, prog_bar=True, logger=True)\n",
    "#         tqdm.write('Dice: \\t%.3f' % mean_loss)\n",
    "        return mean_loss, mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_id_head_list = []\n",
    "for l_ in train_gby['species_id']:\n",
    "    species_id_head_list.append(l_[0])\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/user/.cache/torch/hub/zhanghang1989_ResNeSt_master\n",
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | encoder   | ResNet            | 28.6 M\n",
      "1 | criterion | BCEWithLogitsLoss | 0     \n",
      "------------------------------------------------\n",
      "28.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.6 M    Total params\n",
      "/home/user/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48510ebd450741949a11f43188eb2e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(1.3573, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.1377)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The validation_epoch_end should not return anything as of 9.1. To log, use self.log(...) or self.write(...) directly in the LightningModule\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/user/.local/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d31519c5f64a6cb6492d77470c9883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf36d9167fed43f79b2934c47ed3767a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(1.3485, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.2041)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58eef5d395ea457db4e7440e38ec8cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(1.3230, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.2569)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cedaad1a1d245349d07c570ae1c677e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(1.2462, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.2733)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294e31bf37664e3b992844d9e0044d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(1.1590, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.2826)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc13fd9c123e43d5a70fc6cae283b7cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(1.0934, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.2975)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a46d30459941d49ab2ec1cdd3fb907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(1.1094, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.3185)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316ce47ca6474bb49eaf206cf8e7c834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(1.0603, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.3323)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272bacf47cf74dbe9ab3a0451ccd8997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(1.0351, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.3556)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456da0c50c7243fbbb7d10ee999b7306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.9953, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.3965)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d0c4e7466a440b8fea59066b431dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.9452, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.4520)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32971667eae841258b9996c43ea75206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.9234, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.4667)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e721f00d8c3446a9074edb88a8fd2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.8712, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.5047)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78e23741b974e86b60ed02db0e46067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.8223, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.5465)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ba5ef98461480e95f8f4076746f8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.8092, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.5486)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5d0d89586747e09332d0a6c37a949f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.7441, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.6006)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de53305f68e46248d482b47d5cee201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.7150, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.5822)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db3058b66e147cdaf097a0e77d917a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.7007, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.6097)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a06979b5a74700a6a9fd727fc1a30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.7065, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.6516)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2801c31a5fe746a6b097d8fb564492b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.7006, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.6265)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0582730048274b8786214e60825287c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.6178, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.6666)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd7cd9c231f4c0c979589c5f8185dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.6173, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.6784)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f200a0f84ccf4a3985ce92de62edbf71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.6477, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.7026)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65fe7c98e614e9181717d723478ecf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.6315, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.6877)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585bd7dd53dc4f5793a905ba81d086ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.5916, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.6946)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75362b10f41f4585805e7d5b7e8db185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.5999, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.7032)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8d0b4c0c384b799ec06c5d75aee7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.6198, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.6968)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736539e94eb341d7942611ea8ef67ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.6813, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.6749)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5266da068b043888305d10b62e57886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.6321, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.7433)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1555f8df74484ea786b2a5625528dd9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_epoch_loss =  tensor(0.6043, device='cuda:0')\n",
      "valid_epoch_lwlap =  tensor(0.7354)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold_id, (train_index, val_index) in enumerate(skf.split(train_gby, species_id_head_list)):\n",
    "    # Picking only first fold to train/val on\n",
    "    # This means loss of 20% training data\n",
    "    # To avoid this, you can train 5 different models on 5 folds and average predictions\n",
    "    train_data = train_gby.iloc[train_index]\n",
    "    valid_data = train_gby.iloc[val_index]\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    valid_data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    train_dataset = SpectrogramTrainDataset(train_data, config[\"dataset\"][\"params\"])\n",
    "    valid_dataset = SpectrogramTrainDataset(valid_data, config[\"dataset\"][\"params\"])\n",
    "    \n",
    "    train_loader = data.DataLoader(train_dataset, **config[\"loader\"][\"train\"])\n",
    "    valid_loader = data.DataLoader(valid_dataset, **config[\"loader\"][\"valid\"])\n",
    "    \n",
    "    model = LitModule()\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=EPOCH,\n",
    "        default_root_dir=OUTPUT_DIR,\n",
    "        gpus=1,\n",
    "        callbacks=[early_stop_callback]\n",
    "    )\n",
    "    trainer.fit(model, train_loader, valid_loader)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), OUTPUT_DIR + 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
